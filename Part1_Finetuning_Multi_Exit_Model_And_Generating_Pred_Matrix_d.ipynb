{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZrkmpkW8Wbj"
   },
   "source": [
    "# Part-1 : Section A\n",
    "Training a multi-exit ElasticBERT model on SST-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CkfzdLG9C1T",
    "outputId": "a7400ed9-42af-4197-ba5e-a1b1bb03fa34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '/Desktop/divya/UBERT/MutiExitDNNs/ElasticBERT'\n",
      "C:\\Users\\divya\\Desktop\\divya\\UBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# The code closely follows the original ElasticBERT repository\n",
    "# Feature to train models with a given exit configuration is added\n",
    "# !git clone https://github.com/MLiONS/MutiExitDNNs.git\n",
    "\n",
    "\n",
    "%cd /home/divya/UBERT/MutiExitDNNs/ElasticBERT\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A26ZgAoxPyiJ",
    "outputId": "93510bc8-6217-4989-c9e9-24beef747003"
   },
   "outputs": [],
   "source": [
    "#All the hyper-parameters/ location to training dataset are set in\n",
    "#MultiExitDNNs -> finetune-dynamic -> finetune_elue_entropy.sh file\n",
    "\n",
    "#1)Set the correct location to SST-2 dataset\n",
    "#All models are trained on SST-2 \"train\" split and evaluated on \"dev\" split\n",
    "#\"train.tsv\" and \"dev.tsv\" are expected to be in ELUE_DIR/TASK_NAME\n",
    "#You can set both ELUE_DIR and TASK_NAME in finetune_elue_entropy.sh\n",
    "#Or change the dataset directory using \"data_dir\" option\n",
    "\n",
    "#2)Please change the \"num_output_layers\" option as per the desired exit-configuration\n",
    "\n",
    "#3)Model checkpoints will be saved at \"output_dir\" and\n",
    "#logs will be available at \"log_dir\"\n",
    "# !bash /home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic/finetune_elue_entropy.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tfcWxU28kkl"
   },
   "source": [
    "# Part-1 : Section B\n",
    "Generating the prediction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "acoXLlG2y1ty"
   },
   "outputs": [],
   "source": [
    "#Evaluation on other datasets-IMDb or Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kb9mBUI5y1q-",
    "outputId": "f06bf2bb-9a73-407f-b80d-8b5064d1cc1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 12:48:54.070235: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-27 12:48:54.070271: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer as ElasticBertTokenizer\n",
    "\n",
    "#Set the current directory location inside \"finetune-dynamic\" folder\n",
    "%cd /home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic\n",
    "\n",
    "from models.configuration_elasticbert import ElasticBertConfig\n",
    "from models.modeling_elasticbert_entropy import ElasticBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZkT2IrL2b90d"
   },
   "outputs": [],
   "source": [
    "#Set location to the best performing model\n",
    "#Model checkpoints are saved at \"output_dir\" from Part-1: Section A\n",
    "checkpoint_snli = '/home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic/ckpts/elue/entropy/SNLI/checkpoint-25700'\n",
    "checkpoint_sst = '/home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic/ckpts/elue/entropy/SST-2/checkpoint-300'\n",
    "checkpoint_mrpc = '/home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic/ckpts/elue/entropy/MRPC/checkpoint-575'\n",
    "checkpoint_scitail = '/home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic/ckpts/elue/entropy/SciTail/checkpoint-3690'\n",
    "checkpoint_rte = '/home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic/ckpts/elue/entropy/RTE/checkpoint-390'\n",
    "checkpoint_mnli = '/home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic/ckpts/elue/entropy/MNLI/checkpoint-61360'\n",
    "checkpoint_qnli = '/home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic/ckpts/elue/entropy/QNLI/checkpoint-16370'\n",
    "checkpoint_qqp = '/home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic/ckpts/elue/entropy/QQP/checkpoint-56855'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cCv-3N0CaM0s"
   },
   "outputs": [],
   "source": [
    "config = ElasticBertConfig.from_pretrained(checkpoint_mnli)\n",
    "tokenizer = ElasticBertTokenizer.from_pretrained(checkpoint_mnli)\n",
    "model = ElasticBertForSequenceClassification.from_pretrained(checkpoint_mnli)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AFt7llc_g130"
   },
   "outputs": [],
   "source": [
    "def get_args(arg_vec):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "    parser.add_argument(\n",
    "        \"--num_hidden_layers\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help='The number of layers to import.',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_output_layers\",\n",
    "        nargs = 12,\n",
    "        default=None,\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help='The number of layers to output.',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to pre-trained model or shortcut name.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--task_name\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The name of the task to train selected in the list.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The output directory where the logs will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--spec_eval\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=False,\n",
    "        help=\"'Set as train or test based on specific split on which to evaluate'\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--patience\",\n",
    "        default='0',\n",
    "        type=str,\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--regression_threshold\",\n",
    "        default=0,\n",
    "        type=float,\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--early_exit_entropy\",\n",
    "        default='0.1',\n",
    "        type=str,\n",
    "        required=False,\n",
    "    )\n",
    "    # Other parameters\n",
    "    parser.add_argument(\n",
    "        \"--load\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"The path of ckpts used to continue training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config_name\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Pretrained config name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cache_dir\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_seq_length\",\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "             \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Whether to use debug mode.\")\n",
    "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
    "    parser.add_argument(\n",
    "        \"--evaluate_during_training\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Run evaluation during training at each logging step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--do_lower_case\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Set this flag if you are using an uncased model.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_gpu_train_batch_size\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU/CPU for training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_gpu_eval_batch_size\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU/CPU for evaluation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        default=5e-5,\n",
    "        type=float,\n",
    "        help=\"The initial learning rate for Adam.\",\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        default=3.0,\n",
    "        type=float,\n",
    "        help=\"Total number of training epochs to perform.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_steps\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "    parser.add_argument(\"--warmup_rate\", default=0, type=float, help=\"Linear warmup over warmup_rate.\")\n",
    "\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Save checkpoint every X updates steps.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_all_checkpoints\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    "    )\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_output_dir\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite the content of the output directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_cache\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite the cached training and evaluation sets\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--not_save_model\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Do not save model checkpoints\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=6, help=\"random seed for initialization\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fp16_opt_level\",\n",
    "        type=str,\n",
    "        default=\"O1\",\n",
    "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "             \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--local_rank\",\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        help=\"For distributed training: local_rank\",\n",
    "    )\n",
    "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "    args = parser.parse_args(arg_vec)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "E2cX_ym5pJwl"
   },
   "outputs": [],
   "source": [
    "from load_data import (\n",
    "    load_and_cache_examples_glue,\n",
    "    load_and_cache_examples_elue,\n",
    ")\n",
    "\n",
    "def evaluate_elue_entropy(args, model, tokenizer, prefix=\"\", eval_highway=False, entropy=0.):\n",
    "    model.elasticbert.set_early_exit_entropy(entropy)\n",
    "    model.elasticbert.set_eval_state(eval_highway)\n",
    "    model.elasticbert.reset_stats()\n",
    "\n",
    "    eval_task = args.task_name.lower()\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    num_op_layers = args.num_output_layers\n",
    "\n",
    "    results = {}\n",
    "    results_all = []\n",
    "    exit_layer = []\n",
    "    for i in range(sum(num_op_layers)):\n",
    "        results_all.append({})\n",
    "\n",
    "    if args.spec_eval:\n",
    "      eval_dataset = load_and_cache_examples_elue(args, eval_task, tokenizer, data_type=args.spec_eval)\n",
    "    else:\n",
    "      eval_dataset = load_and_cache_examples_elue(args, eval_task, tokenizer, data_type='train')\n",
    "\n",
    "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    # multi-gpu eval\n",
    "    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    preds_all = []\n",
    "    pred_tuple = []\n",
    "    for i in range(sum(num_op_layers)):\n",
    "        preds_all.append(None)\n",
    "        pred_tuple.append(None)\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"labels\": batch[-1],\n",
    "            }\n",
    "            inputs[\"token_type_ids\"] = batch[2]\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if out_label_ids is None:\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "        if not eval_highway:\n",
    "            for i, pred in enumerate(preds_all):\n",
    "                if pred is None:\n",
    "                    preds_all[i] = logits[i].detach().cpu().numpy()\n",
    "                else:\n",
    "                    preds_all[i] = np.append(pred, logits[i].detach().cpu().numpy(), axis=0)\n",
    "        else:\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args.output_mode == \"classification\":\n",
    "        if not eval_highway:\n",
    "            for i, pred in enumerate(preds_all):\n",
    "                preds_all[i] = np.argmax(pred, axis = 1)\n",
    "                pred_tuple[i] = pred\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis = 1)\n",
    "            pred_tuple[i] = pred\n",
    "\n",
    "    elif args.output_mode == \"regression\":\n",
    "        if not eval_highway:\n",
    "            for i, pred in enumerate(preds_all):\n",
    "                preds_all[i] = np.squeeze(pred)\n",
    "        else:\n",
    "            preds = np.squeeze(preds)\n",
    "\n",
    "    if not eval_highway:\n",
    "        for i, pred in enumerate(preds_all):\n",
    "            if eval_task == 'rte' or 'qnli' or 'wnli' or 'qqp':\n",
    "                eval_task = 'scitail'\n",
    "            if eval_task == 'mnli':\n",
    "                eval_task = 'snli'\n",
    "            if eval_task == 'yelp':\n",
    "              eval_task = 'imdb'\n",
    "            result = elue_compute_metrics(eval_task, pred, out_label_ids)\n",
    "            results_all[i].update(result)\n",
    "\n",
    "    else:\n",
    "        if eval_task == 'rte' or 'qnli' or 'wnli' or 'qqp':\n",
    "                eval_task = 'scitail'\n",
    "        if eval_task == 'mnli':\n",
    "                eval_task = 'snli'\n",
    "        result = elue_compute_metrics(eval_task, preds, out_label_ids)\n",
    "        results.update(result)\n",
    "\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            print(\"  %s = %s\" % (key, str(result[key])))\n",
    "\n",
    "        exiting_layer_every_ins = model.elasticbert.exiting_layer_every_ins\n",
    "        exit_layer.append(exiting_layer_every_ins)\n",
    "\n",
    "    if eval_highway:\n",
    "        speed_up = model.elasticbert.log_stats()\n",
    "        return results, speed_up, exit_layer\n",
    "\n",
    "    if args.spec_eval:\n",
    "      return results_all, preds_all, pred_tuple, out_label_ids\n",
    "\n",
    "    return results_all, preds_all, pred_tuple , out_label_ids\n",
    "    #return results_all, preds_all, out_label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jCJmL5-yez21"
   },
   "outputs": [],
   "source": [
    "ELUE_DIR='/home/divya/UBERT/elue_data'\n",
    "TASK_NAME='MNLI'\n",
    "\n",
    "arg_vec= ['--model_name_or_path', 'fnlp/elasticbert-base',\n",
    "  '--task_name', 'MNLI', \\\n",
    "  '--do_train', \\\n",
    "  '--do_lower_case', \\\n",
    "  '--data_dir', \"/home/divya/UBERT/elue_data\", \\\n",
    "  '--log_dir', '/home/divya/UBERT/ElasticBERT/logs/elue/entropy/SNLI-BTestCheck', \\\n",
    "  '--output_dir', '/home/divya/UBERT/ElasticBERT/ckpts/elue/entropy/SNLI-BTestCheck', \\\n",
    "  '--num_hidden_layers', '12', \\\n",
    "  '--num_output_layers', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '1', \\\n",
    "  '--max_seq_length', '128', \\\n",
    "  '--per_gpu_train_batch_size', '32', \\\n",
    "  '--per_gpu_eval_batch_size',' 32', \\\n",
    "  '--learning_rate', '2e-5', \\\n",
    "  '--weight_decay', '0.1', \\\n",
    "  '--save_steps', '50', \\\n",
    "  '--logging_steps', '50', \\\n",
    "  '--num_train_epochs', '5',  \\\n",
    "  '--warmup_rate', '0.06', \\\n",
    "  '--evaluate_during_training', \\\n",
    "  '--overwrite_output_dir'\n",
    "]\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "args = get_args(arg_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLZpjqakoP9G",
    "outputId": "a9b77e92-8aaf-4f06-db5f-b87dd9aec42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticBertForSequenceClassification(\n",
       "  (elasticbert): ElasticBertModel(\n",
       "    (embeddings): ElasticBertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElasticBertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElasticBertLayer(\n",
       "          (attention): ElasticBertAttention(\n",
       "            (self): ElasticBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElasticBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElasticBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElasticBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): ModuleList(\n",
       "        (0): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (1): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (2): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (3): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (4): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (5): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (6): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (7): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (8): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (9): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (10): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (11): ElasticBertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifiers): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (1): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (2): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (3): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (4): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (5): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (6): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (7): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (8): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (9): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (10): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (11): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "args.output_mode = 'classification'\n",
    "\n",
    "print(args.device)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "50EjaE5MhPkU"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/hsm207/imdb_data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lfKacfaChWSJ"
   },
   "outputs": [],
   "source": [
    "# %cd imdb_data\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YsgrccgxiiQq"
   },
   "outputs": [],
   "source": [
    "# !tf_upgrade_v2 --infile create_imdb_dataset.py --outfile bar.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vbLkqld8ipqI"
   },
   "outputs": [],
   "source": [
    "# !python bar.py --output_dir /home/divya/UBERT/elue_data/imdb_data/imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rNSuHnH-mSPt"
   },
   "outputs": [],
   "source": [
    "#Custom Selection\n",
    "dataset = 'SNLI'#'IMDb' #or 'Yelp'\n",
    "\n",
    "#To check model performance on SST-2 dev split:\n",
    "#Please set dataset = 'SST-2' and data_split='dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZyrFjs52hwRW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_preds(eval_dataset='SNLI', data_split='train'):\n",
    "  args.spec_eval = False\n",
    "  args.task_name=eval_dataset.lower()\n",
    "  args.data_dir=ELUE_DIR + '/'+args.task_name\n",
    "\n",
    "  results_all, exit_preds, pred_tuple, op_labels = evaluate_elue_entropy(args, model, tokenizer)\n",
    "\n",
    "\n",
    "  # exit_preds_list = np.stack(exit_preds, axis=1)\n",
    "  # df = pd.DataFrame((exit_preds_list) )\n",
    "  # df['op_labels'] = op_labels\n",
    "\n",
    "  return  results_all, exit_preds, pred_tuple, op_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxJkyv_xHy-H",
    "outputId": "e676791a-51ed-49db-a84a-6e2a00c290e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 17168/17168 [26:47<00:00, 10.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from elue import elue_compute_metrics\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "results, final_preds, pred_tuple, op_labels = get_preds(eval_dataset=dataset, data_split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(pred_tuple[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FnF4zrhPMYNV"
   },
   "outputs": [],
   "source": [
    "# accurac_imd = []\n",
    "# for j in range(12):\n",
    "#     accuracy = 0\n",
    "#     for i in range(len(op_labels)):\n",
    "#         if final_preds[j][i] == op_labels[i]:\n",
    "#             accuracy+=1\n",
    "#         else:\n",
    "#             pass\n",
    "#     accurac_imd.append(accuracy/len(op_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accurac_yelp = []\n",
    "# for j in range(12):\n",
    "#     accuracy = 0\n",
    "#     for i in range(len(op_labels)):\n",
    "#         if final_preds[j][i] == op_labels[i]:\n",
    "#             accuracy+=1\n",
    "#         else:\n",
    "#             pass\n",
    "#     accurac_yelp.append(accuracy/len(op_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "# plt.plot(x, accurac_imd, linestyle='dotted', marker = '^', color=\"r\")\n",
    "# plt.plot(x, accurac_yelp, linestyle='dotted', marker = 'o', color=\"g\")\n",
    "# plt.xlabel(\"Layer number\", fontsize = 17)\n",
    "# plt.ylabel(\"Accuracy\", fontsize = 17)\n",
    "# plt.title(\"Accuracy vs Layer\", fontsize = 17)\n",
    "# plt.legend(['IMDb', 'Yelp'],\n",
    "#         prop = {'size' : 15},\n",
    "#         loc = 'lower right', shadow = True,\n",
    "#         facecolor = 'white')\n",
    "# plt.xticks(fontsize=13)\n",
    "# plt.yticks(fontsize=13)\n",
    "# plt.savefig(\"/home/divya/UBERT/Result_plot/Accuracy_vs_layer_Imdb and yelp.pdf\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dqNuO_askX0X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "def softmax(x):\n",
    "    return(np.exp(x)/np.exp(x).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_thi = []\n",
    "for i in range(len(pred_tuple[0])):\n",
    "    pred_prob_thi.append(max(softmax(pred_tuple[2][i])))\n",
    "# pred_proba_thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uX8DlkDGWdQ-",
    "outputId": "639a3136-cd41-4455-e370-1d02f7e01204"
   },
   "outputs": [],
   "source": [
    "pred_prob_six = []\n",
    "for i in range(len(pred_tuple[0])):\n",
    "    pred_prob_six.append(max(softmax(pred_tuple[5][i])))\n",
    "# pred_proba_fou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99910575,\n",
       " 0.9994923,\n",
       " 0.9529833,\n",
       " 0.96982104,\n",
       " 0.98246974,\n",
       " 0.99934644,\n",
       " 0.90339124,\n",
       " 0.98659974,\n",
       " 0.9907664,\n",
       " 0.7798163,\n",
       " 0.9981209,\n",
       " 0.9753172,\n",
       " 0.997316,\n",
       " 0.99983996,\n",
       " 0.98320127,\n",
       " 0.9926979,\n",
       " 0.9994368,\n",
       " 0.73188215,\n",
       " 0.9937423,\n",
       " 0.9989383,\n",
       " 0.98298115,\n",
       " 0.994629,\n",
       " 0.9995364,\n",
       " 0.98732424,\n",
       " 0.9991029,\n",
       " 0.6196092,\n",
       " 0.81272244,\n",
       " 0.997546,\n",
       " 0.99068004,\n",
       " 0.9963358,\n",
       " 0.81837183,\n",
       " 0.9871621,\n",
       " 0.9980643,\n",
       " 0.5162542,\n",
       " 0.96856296,\n",
       " 0.9967236,\n",
       " 0.98666567,\n",
       " 0.99731016,\n",
       " 0.99883467,\n",
       " 0.9669747,\n",
       " 0.92535925,\n",
       " 0.99955,\n",
       " 0.8710396,\n",
       " 0.9536737,\n",
       " 0.9972004,\n",
       " 0.9927417,\n",
       " 0.9981533,\n",
       " 0.9980363,\n",
       " 0.99644864,\n",
       " 0.47906926,\n",
       " 0.9211222,\n",
       " 0.99983925,\n",
       " 0.9974528,\n",
       " 0.97958136,\n",
       " 0.9222829,\n",
       " 0.6592883,\n",
       " 0.9963254,\n",
       " 0.99809754,\n",
       " 0.9966623,\n",
       " 0.97587,\n",
       " 0.95984626,\n",
       " 0.9932312,\n",
       " 0.9685359,\n",
       " 0.9912516,\n",
       " 0.9539883,\n",
       " 0.8702022,\n",
       " 0.9243329,\n",
       " 0.99241656,\n",
       " 0.9993096,\n",
       " 0.9988882,\n",
       " 0.95071775,\n",
       " 0.6482637,\n",
       " 0.9972621,\n",
       " 0.6541429,\n",
       " 0.8965362,\n",
       " 0.78879297,\n",
       " 0.99505806,\n",
       " 0.998071,\n",
       " 0.9947783,\n",
       " 0.9926926,\n",
       " 0.99054146,\n",
       " 0.93221927,\n",
       " 0.993588,\n",
       " 0.9966385,\n",
       " 0.9942974,\n",
       " 0.998178,\n",
       " 0.90694207,\n",
       " 0.99938715,\n",
       " 0.98093253,\n",
       " 0.81407875,\n",
       " 0.9995969,\n",
       " 0.9896897,\n",
       " 0.9934349,\n",
       " 0.97719026,\n",
       " 0.9969281,\n",
       " 0.95417976,\n",
       " 0.83451426,\n",
       " 0.9517906,\n",
       " 0.8221678,\n",
       " 0.9780016,\n",
       " 0.5401454,\n",
       " 0.9977421,\n",
       " 0.97542506,\n",
       " 0.9722889,\n",
       " 0.8956946,\n",
       " 0.9027809,\n",
       " 0.9932566,\n",
       " 0.9989886,\n",
       " 0.99975866,\n",
       " 0.71187335,\n",
       " 0.99861217,\n",
       " 0.9954747,\n",
       " 0.9960135,\n",
       " 0.99985087,\n",
       " 0.988802,\n",
       " 0.7759608,\n",
       " 0.79458845,\n",
       " 0.9944041,\n",
       " 0.9874086,\n",
       " 0.9961261,\n",
       " 0.99953574,\n",
       " 0.9599229,\n",
       " 0.9837036,\n",
       " 0.99693376,\n",
       " 0.9122245,\n",
       " 0.8726104,\n",
       " 0.9553887,\n",
       " 0.9920205,\n",
       " 0.99933505,\n",
       " 0.89509994,\n",
       " 0.9525918,\n",
       " 0.99619836,\n",
       " 0.9962783,\n",
       " 0.7854961,\n",
       " 0.9995917,\n",
       " 0.59639597,\n",
       " 0.9990572,\n",
       " 0.9618342,\n",
       " 0.70464677,\n",
       " 0.75907344,\n",
       " 0.54512393,\n",
       " 0.98864883,\n",
       " 0.8766053,\n",
       " 0.934685,\n",
       " 0.98936284,\n",
       " 0.9995078,\n",
       " 0.8138721,\n",
       " 0.99333686,\n",
       " 0.92210025,\n",
       " 0.88973844,\n",
       " 0.9971493,\n",
       " 0.99803776,\n",
       " 0.99927485,\n",
       " 0.9989525,\n",
       " 0.98588,\n",
       " 0.82800215,\n",
       " 0.98591214,\n",
       " 0.9844152,\n",
       " 0.94242865,\n",
       " 0.9987411,\n",
       " 0.8091208,\n",
       " 0.97172886,\n",
       " 0.85094076,\n",
       " 0.8334583,\n",
       " 0.99898785,\n",
       " 0.986647,\n",
       " 0.986114,\n",
       " 0.9996534,\n",
       " 0.98671275,\n",
       " 0.997232,\n",
       " 0.9899514,\n",
       " 0.88916487,\n",
       " 0.99474806,\n",
       " 0.9830297,\n",
       " 0.9791598,\n",
       " 0.9847553,\n",
       " 0.3595054,\n",
       " 0.99558556,\n",
       " 0.52448684,\n",
       " 0.98703736,\n",
       " 0.999859,\n",
       " 0.99472356,\n",
       " 0.999124,\n",
       " 0.97931653,\n",
       " 0.98969376,\n",
       " 0.6229646,\n",
       " 0.9405633,\n",
       " 0.9868957,\n",
       " 0.978016,\n",
       " 0.9195562,\n",
       " 0.9871589,\n",
       " 0.47985476,\n",
       " 0.99692976,\n",
       " 0.9994899,\n",
       " 0.7543253,\n",
       " 0.87481976,\n",
       " 0.9856545,\n",
       " 0.9992788,\n",
       " 0.849051,\n",
       " 0.9550905,\n",
       " 0.9980689,\n",
       " 0.99849385,\n",
       " 0.94366336,\n",
       " 0.96180993,\n",
       " 0.8631399,\n",
       " 0.99433476,\n",
       " 0.99971706,\n",
       " 0.9737885,\n",
       " 0.9345124,\n",
       " 0.9930878,\n",
       " 0.9891215,\n",
       " 0.57533556,\n",
       " 0.99894947,\n",
       " 0.9977799,\n",
       " 0.58972174,\n",
       " 0.9998213,\n",
       " 0.99675936,\n",
       " 0.5544057,\n",
       " 0.9608557,\n",
       " 0.96948695,\n",
       " 0.9852141,\n",
       " 0.99755865,\n",
       " 0.9764019,\n",
       " 0.70108545,\n",
       " 0.42636386,\n",
       " 0.97897285,\n",
       " 0.9898513,\n",
       " 0.51918536,\n",
       " 0.7898362,\n",
       " 0.9997464,\n",
       " 0.98848265,\n",
       " 0.9391478,\n",
       " 0.9208665,\n",
       " 0.6706298,\n",
       " 0.99878436,\n",
       " 0.9993827,\n",
       " 0.99886453,\n",
       " 0.99832904,\n",
       " 0.90335256,\n",
       " 0.99943954,\n",
       " 0.7165457,\n",
       " 0.95158565,\n",
       " 0.8617923,\n",
       " 0.9998087,\n",
       " 0.9920326,\n",
       " 0.9996111,\n",
       " 0.998283,\n",
       " 0.9987513,\n",
       " 0.9959658,\n",
       " 0.9912929,\n",
       " 0.99841094,\n",
       " 0.8995864,\n",
       " 0.918044,\n",
       " 0.99979,\n",
       " 0.99869955,\n",
       " 0.98619384,\n",
       " 0.99061126,\n",
       " 0.99761313,\n",
       " 0.9834903,\n",
       " 0.9906677,\n",
       " 0.9995268,\n",
       " 0.997476,\n",
       " 0.6260615,\n",
       " 0.9996563,\n",
       " 0.9918946,\n",
       " 0.99719024,\n",
       " 0.84153163,\n",
       " 0.9916803,\n",
       " 0.7452667,\n",
       " 0.9400569,\n",
       " 0.99125916,\n",
       " 0.99939847,\n",
       " 0.97341496,\n",
       " 0.9988504,\n",
       " 0.79868424,\n",
       " 0.99818665,\n",
       " 0.51967746,\n",
       " 0.9958931,\n",
       " 0.64576125,\n",
       " 0.7416971,\n",
       " 0.7400591,\n",
       " 0.9983106,\n",
       " 0.9965887,\n",
       " 0.9718376,\n",
       " 0.82366246,\n",
       " 0.79937243,\n",
       " 0.9986253,\n",
       " 0.9164295,\n",
       " 0.99818057,\n",
       " 0.995901,\n",
       " 0.9988047,\n",
       " 0.65552795,\n",
       " 0.7203117,\n",
       " 0.9961091,\n",
       " 0.9995357,\n",
       " 0.99966884,\n",
       " 0.99653995,\n",
       " 0.99935377,\n",
       " 0.9794839,\n",
       " 0.9782046,\n",
       " 0.9963501,\n",
       " 0.9985066,\n",
       " 0.99383974,\n",
       " 0.95179576,\n",
       " 0.7067799,\n",
       " 0.994553,\n",
       " 0.9965703,\n",
       " 0.70558643,\n",
       " 0.9964866,\n",
       " 0.86431456,\n",
       " 0.99987376,\n",
       " 0.99673647,\n",
       " 0.9856446,\n",
       " 0.9889433,\n",
       " 0.9950039,\n",
       " 0.9998543,\n",
       " 0.9825717,\n",
       " 0.9858954,\n",
       " 0.96043545,\n",
       " 0.9998172,\n",
       " 0.9970083,\n",
       " 0.99723923,\n",
       " 0.8905327,\n",
       " 0.98055816,\n",
       " 0.9993653,\n",
       " 0.8586327,\n",
       " 0.6927231,\n",
       " 0.8059919,\n",
       " 0.9983513,\n",
       " 0.9611162,\n",
       " 0.99825615,\n",
       " 0.9882304,\n",
       " 0.53914446,\n",
       " 0.9866969,\n",
       " 0.88760567,\n",
       " 0.8679468,\n",
       " 0.93327355,\n",
       " 0.99621516,\n",
       " 0.9875101,\n",
       " 0.99296427,\n",
       " 0.62055933,\n",
       " 0.9986593,\n",
       " 0.94625044,\n",
       " 0.969882,\n",
       " 0.9996278,\n",
       " 0.98970294,\n",
       " 0.55247664,\n",
       " 0.94719553,\n",
       " 0.9982934,\n",
       " 0.9767681,\n",
       " 0.9914746,\n",
       " 0.8149295,\n",
       " 0.9930224,\n",
       " 0.99740976,\n",
       " 0.9991772,\n",
       " 0.9269531,\n",
       " 0.7187647,\n",
       " 0.6191337,\n",
       " 0.99917316,\n",
       " 0.79927546,\n",
       " 0.9380456,\n",
       " 0.9600464,\n",
       " 0.97955304,\n",
       " 0.9946776,\n",
       " 0.59529585,\n",
       " 0.9874035,\n",
       " 0.99946177,\n",
       " 0.94617575,\n",
       " 0.9770532,\n",
       " 0.99911886,\n",
       " 0.99410707,\n",
       " 0.5263232,\n",
       " 0.9978392,\n",
       " 0.9995969,\n",
       " 0.9805159,\n",
       " 0.988351,\n",
       " 0.9994031,\n",
       " 0.9899663,\n",
       " 0.96523756,\n",
       " 0.99530256,\n",
       " 0.93950963,\n",
       " 0.9558536,\n",
       " 0.99968636,\n",
       " 0.9484751,\n",
       " 0.99576557,\n",
       " 0.97726905,\n",
       " 0.99494135,\n",
       " 0.99957067,\n",
       " 0.9874115,\n",
       " 0.89676297,\n",
       " 0.99936897,\n",
       " 0.52928174,\n",
       " 0.999854,\n",
       " 0.8482039,\n",
       " 0.9987334,\n",
       " 0.94303757,\n",
       " 0.9476878,\n",
       " 0.75368637,\n",
       " 0.5905103,\n",
       " 0.9980774,\n",
       " 0.95720655,\n",
       " 0.9948171,\n",
       " 0.87697965,\n",
       " 0.9916546,\n",
       " 0.9658942,\n",
       " 0.9942923,\n",
       " 0.99952364,\n",
       " 0.97211075,\n",
       " 0.97742814,\n",
       " 0.68180144,\n",
       " 0.99842924,\n",
       " 0.99227923,\n",
       " 0.99929976,\n",
       " 0.8625566,\n",
       " 0.99499893,\n",
       " 0.5028889,\n",
       " 0.98418313,\n",
       " 0.9963124,\n",
       " 0.99162376,\n",
       " 0.9241165,\n",
       " 0.9925952,\n",
       " 0.98670465,\n",
       " 0.99599737,\n",
       " 0.97528225,\n",
       " 0.9928641,\n",
       " 0.925482,\n",
       " 0.56570476,\n",
       " 0.99873155,\n",
       " 0.9813464,\n",
       " 0.9978255,\n",
       " 0.9989209,\n",
       " 0.9997317,\n",
       " 0.91910017,\n",
       " 0.6240146,\n",
       " 0.8166112,\n",
       " 0.74753976,\n",
       " 0.99442494,\n",
       " 0.9819515,\n",
       " 0.9903132,\n",
       " 0.9995888,\n",
       " 0.9978186,\n",
       " 0.99922466,\n",
       " 0.8688228,\n",
       " 0.8971497,\n",
       " 0.88521284,\n",
       " 0.91181046,\n",
       " 0.93568224,\n",
       " 0.9996148,\n",
       " 0.9950283,\n",
       " 0.99961317,\n",
       " 0.99074,\n",
       " 0.41733253,\n",
       " 0.99970615,\n",
       " 0.9946854,\n",
       " 0.64883715,\n",
       " 0.87276185,\n",
       " 0.9979331,\n",
       " 0.93707556,\n",
       " 0.9710529,\n",
       " 0.9908469,\n",
       " 0.88019574,\n",
       " 0.9884031,\n",
       " 0.9996235,\n",
       " 0.99967,\n",
       " 0.5658936,\n",
       " 0.93120277,\n",
       " 0.9201975,\n",
       " 0.6822747,\n",
       " 0.89950615,\n",
       " 0.9944509,\n",
       " 0.9997111,\n",
       " 0.91437125,\n",
       " 0.9973552,\n",
       " 0.9888048,\n",
       " 0.86729825,\n",
       " 0.9155692,\n",
       " 0.99939203,\n",
       " 0.5472505,\n",
       " 0.99659956,\n",
       " 0.5179814,\n",
       " 0.4457673,\n",
       " 0.99737126,\n",
       " 0.9352124,\n",
       " 0.96058,\n",
       " 0.9872134,\n",
       " 0.8976609,\n",
       " 0.9897975,\n",
       " 0.65017617,\n",
       " 0.8671734,\n",
       " 0.9529055,\n",
       " 0.9864548,\n",
       " 0.9991731,\n",
       " 0.9965596,\n",
       " 0.9844178,\n",
       " 0.9893209,\n",
       " 0.9643399,\n",
       " 0.99689597,\n",
       " 0.9996101,\n",
       " 0.62520576,\n",
       " 0.8257217,\n",
       " 0.7699159,\n",
       " 0.5022688,\n",
       " 0.9819716,\n",
       " 0.9550908,\n",
       " 0.986616,\n",
       " 0.82354003,\n",
       " 0.9808169,\n",
       " 0.96409,\n",
       " 0.67239606,\n",
       " 0.9886824,\n",
       " 0.99400204,\n",
       " 0.81629264,\n",
       " 0.98666734,\n",
       " 0.8851865,\n",
       " 0.63925797,\n",
       " 0.99661857,\n",
       " 0.99358684,\n",
       " 0.99942684,\n",
       " 0.99596536,\n",
       " 0.99101496,\n",
       " 0.99117845,\n",
       " 0.9983627,\n",
       " 0.9779296,\n",
       " 0.9977513,\n",
       " 0.9795937,\n",
       " 0.98574734,\n",
       " 0.9993924,\n",
       " 0.61187196,\n",
       " 0.53609306,\n",
       " 0.9993457,\n",
       " 0.9872395,\n",
       " 0.9985343,\n",
       " 0.97739995,\n",
       " 0.9837135,\n",
       " 0.9964232,\n",
       " 0.88191396,\n",
       " 0.9981575,\n",
       " 0.9962193,\n",
       " 0.9644392,\n",
       " 0.9981653,\n",
       " 0.99569905,\n",
       " 0.999318,\n",
       " 0.9994078,\n",
       " 0.999254,\n",
       " 0.9936675,\n",
       " 0.99038947,\n",
       " 0.90857905,\n",
       " 0.75057036,\n",
       " 0.55296713,\n",
       " 0.99903095,\n",
       " 0.9801734,\n",
       " 0.88530797,\n",
       " 0.89404666,\n",
       " 0.82722384,\n",
       " 0.60168815,\n",
       " 0.653895,\n",
       " 0.89240706,\n",
       " 0.9716359,\n",
       " 0.999512,\n",
       " 0.9988475,\n",
       " 0.6283839,\n",
       " 0.64286774,\n",
       " 0.99944615,\n",
       " 0.83463806,\n",
       " 0.5992091,\n",
       " 0.9991216,\n",
       " 0.8915547,\n",
       " 0.99609804,\n",
       " 0.9930319,\n",
       " 0.9996093,\n",
       " 0.9992088,\n",
       " 0.9739917,\n",
       " 0.91108364,\n",
       " 0.8465517,\n",
       " 0.9957245,\n",
       " 0.8914417,\n",
       " 0.9926862,\n",
       " 0.56911546,\n",
       " 0.9928826,\n",
       " 0.99962974,\n",
       " 0.92368066,\n",
       " 0.9339375,\n",
       " 0.9988919,\n",
       " 0.9974793,\n",
       " 0.99977255,\n",
       " 0.8768319,\n",
       " 0.48698184,\n",
       " 0.9330787,\n",
       " 0.9964107,\n",
       " 0.9528979,\n",
       " 0.9965168,\n",
       " 0.99665046,\n",
       " 0.98186123,\n",
       " 0.9985744,\n",
       " 0.9940271,\n",
       " 0.6295614,\n",
       " 0.94997424,\n",
       " 0.99927616,\n",
       " 0.9980487,\n",
       " 0.95707846,\n",
       " 0.98265046,\n",
       " 0.99174666,\n",
       " 0.92598367,\n",
       " 0.9982491,\n",
       " 0.8763542,\n",
       " 0.98130435,\n",
       " 0.9792028,\n",
       " 0.99640846,\n",
       " 0.997245,\n",
       " 0.9966111,\n",
       " 0.9807176,\n",
       " 0.59033567,\n",
       " 0.98643035,\n",
       " 0.9960452,\n",
       " 0.9834097,\n",
       " 0.5314433,\n",
       " 0.9434916,\n",
       " 0.85676134,\n",
       " 0.9980487,\n",
       " 0.99625677,\n",
       " 0.5453524,\n",
       " 0.9990191,\n",
       " 0.95333236,\n",
       " 0.9767868,\n",
       " 0.97892636,\n",
       " 0.99238396,\n",
       " 0.99722683,\n",
       " 0.99147004,\n",
       " 0.9729458,\n",
       " 0.9990896,\n",
       " 0.9769492,\n",
       " 0.9771368,\n",
       " 0.98097914,\n",
       " 0.98274916,\n",
       " 0.9042003,\n",
       " 0.9800629,\n",
       " 0.6036002,\n",
       " 0.90693873,\n",
       " 0.9886494,\n",
       " 0.9983072,\n",
       " 0.96717274,\n",
       " 0.77546865,\n",
       " 0.9980817,\n",
       " 0.9998334,\n",
       " 0.96108013,\n",
       " 0.9394941,\n",
       " 0.9998791,\n",
       " 0.9989261,\n",
       " 0.9552346,\n",
       " 0.9981808,\n",
       " 0.99954575,\n",
       " 0.91963154,\n",
       " 0.97556984,\n",
       " 0.99365246,\n",
       " 0.9981245,\n",
       " 0.9997923,\n",
       " 0.68759173,\n",
       " 0.9897597,\n",
       " 0.99933636,\n",
       " 0.9780836,\n",
       " 0.97580725,\n",
       " 0.99775803,\n",
       " 0.78912765,\n",
       " 0.9649565,\n",
       " 0.56305015,\n",
       " 0.99606365,\n",
       " 0.9989028,\n",
       " 0.8900924,\n",
       " 0.95809835,\n",
       " 0.79326373,\n",
       " 0.8443495,\n",
       " 0.7459826,\n",
       " 0.9976054,\n",
       " 0.74209565,\n",
       " 0.99277025,\n",
       " 0.9923162,\n",
       " 0.9960272,\n",
       " 0.9979278,\n",
       " 0.9986209,\n",
       " 0.58994067,\n",
       " 0.94025683,\n",
       " 0.86609894,\n",
       " 0.77325934,\n",
       " 0.9969479,\n",
       " 0.9980109,\n",
       " 0.98447627,\n",
       " 0.7199926,\n",
       " 0.9988872,\n",
       " 0.9958296,\n",
       " 0.99969655,\n",
       " 0.6931254,\n",
       " 0.5624991,\n",
       " 0.9792381,\n",
       " 0.99762136,\n",
       " 0.96048933,\n",
       " 0.966163,\n",
       " 0.99783146,\n",
       " 0.9919918,\n",
       " 0.49972257,\n",
       " 0.99324805,\n",
       " 0.9543367,\n",
       " 0.99243623,\n",
       " 0.9798403,\n",
       " 0.99829394,\n",
       " 0.9651523,\n",
       " 0.93806916,\n",
       " 0.98615986,\n",
       " 0.99015427,\n",
       " 0.73356473,\n",
       " 0.9325016,\n",
       " 0.99707526,\n",
       " 0.9994389,\n",
       " 0.61351776,\n",
       " 0.99437696,\n",
       " 0.97400147,\n",
       " 0.9072041,\n",
       " 0.6123596,\n",
       " 0.95199484,\n",
       " 0.9912954,\n",
       " 0.4963345,\n",
       " 0.99420947,\n",
       " 0.99981314,\n",
       " 0.9982649,\n",
       " 0.9850469,\n",
       " 0.99943715,\n",
       " 0.9924868,\n",
       " 0.99845296,\n",
       " 0.980098,\n",
       " 0.7334798,\n",
       " 0.9997177,\n",
       " 0.99905884,\n",
       " 0.9965729,\n",
       " 0.7043348,\n",
       " 0.50964403,\n",
       " 0.9485185,\n",
       " 0.99520874,\n",
       " 0.9831226,\n",
       " 0.76061904,\n",
       " 0.9910679,\n",
       " 0.80397815,\n",
       " 0.98286474,\n",
       " 0.99977016,\n",
       " 0.98565394,\n",
       " 0.9984501,\n",
       " 0.99841195,\n",
       " 0.9991369,\n",
       " 0.99473006,\n",
       " 0.9997937,\n",
       " 0.9989937,\n",
       " 0.9737296,\n",
       " 0.793865,\n",
       " 0.999805,\n",
       " 0.7498946,\n",
       " 0.9951438,\n",
       " 0.93579715,\n",
       " 0.99935806,\n",
       " 0.9933121,\n",
       " 0.9238664,\n",
       " 0.9994594,\n",
       " 0.72911197,\n",
       " 0.99227,\n",
       " 0.9992741,\n",
       " 0.9871495,\n",
       " 0.99949336,\n",
       " 0.99321353,\n",
       " 0.99489355,\n",
       " 0.9990599,\n",
       " 0.99975675,\n",
       " 0.8900752,\n",
       " 0.95752287,\n",
       " 0.92581844,\n",
       " 0.9606824,\n",
       " 0.99932516,\n",
       " 0.9969149,\n",
       " 0.9966932,\n",
       " 0.99549025,\n",
       " 0.9906914,\n",
       " 0.9981277,\n",
       " 0.84037036,\n",
       " 0.9031014,\n",
       " 0.6449301,\n",
       " 0.99568766,\n",
       " 0.9814523,\n",
       " 0.992977,\n",
       " 0.9955095,\n",
       " 0.99718004,\n",
       " 0.9973294,\n",
       " 0.9151384,\n",
       " 0.9946735,\n",
       " 0.9591653,\n",
       " 0.97430056,\n",
       " 0.9987353,\n",
       " 0.9193055,\n",
       " 0.9924947,\n",
       " 0.9988753,\n",
       " 0.9993488,\n",
       " 0.99749607,\n",
       " 0.94160926,\n",
       " 0.9986716,\n",
       " 0.9841542,\n",
       " 0.99782157,\n",
       " 0.95290345,\n",
       " 0.9987668,\n",
       " 0.9821068,\n",
       " 0.9665279,\n",
       " 0.9063527,\n",
       " 0.9957366,\n",
       " 0.9712665,\n",
       " 0.9813581,\n",
       " 0.9563664,\n",
       " 0.67314744,\n",
       " 0.99726266,\n",
       " 0.9997625,\n",
       " 0.96445733,\n",
       " 0.9991726,\n",
       " 0.9262551,\n",
       " 0.9536004,\n",
       " 0.9726523,\n",
       " 0.99229634,\n",
       " 0.99825436,\n",
       " 0.98363334,\n",
       " 0.9996328,\n",
       " 0.9971735,\n",
       " 0.99856687,\n",
       " 0.4028215,\n",
       " 0.99915683,\n",
       " 0.99971646,\n",
       " 0.9990474,\n",
       " 0.99650055,\n",
       " 0.9973521,\n",
       " 0.95214206,\n",
       " 0.9991411,\n",
       " 0.9860954,\n",
       " 0.8410449,\n",
       " 0.99235004,\n",
       " 0.9983314,\n",
       " 0.9955998,\n",
       " 0.99638236,\n",
       " 0.9981465,\n",
       " 0.99793416,\n",
       " 0.9938602,\n",
       " 0.99958664,\n",
       " 0.9958562,\n",
       " 0.71799475,\n",
       " 0.99726325,\n",
       " 0.5036443,\n",
       " 0.99260086,\n",
       " 0.99950653,\n",
       " 0.9975754,\n",
       " 0.99949914,\n",
       " 0.883549,\n",
       " 0.971078,\n",
       " 0.9997387,\n",
       " 0.86565876,\n",
       " 0.998955,\n",
       " 0.9996419,\n",
       " 0.7061844,\n",
       " 0.99640155,\n",
       " 0.9967757,\n",
       " 0.8124706,\n",
       " 0.99724674,\n",
       " 0.98678404,\n",
       " 0.9577075,\n",
       " 0.55290633,\n",
       " 0.9915464,\n",
       " 0.99238616,\n",
       " 0.99927855,\n",
       " 0.9993622,\n",
       " 0.9993742,\n",
       " 0.9996428,\n",
       " 0.9873827,\n",
       " 0.9997955,\n",
       " 0.99785084,\n",
       " 0.8209977,\n",
       " 0.9908815,\n",
       " 0.8358449,\n",
       " 0.98944646,\n",
       " 0.9975414,\n",
       " 0.99281645,\n",
       " 0.92438394,\n",
       " 0.99942464,\n",
       " 0.6339788,\n",
       " 0.99123126,\n",
       " 0.9883671,\n",
       " 0.9993878,\n",
       " 0.97517663,\n",
       " 0.9961547,\n",
       " 0.99437827,\n",
       " 0.73478633,\n",
       " 0.6608725,\n",
       " 0.509139,\n",
       " 0.9976764,\n",
       " 0.9391227,\n",
       " 0.9257708,\n",
       " 0.99863225,\n",
       " 0.77632916,\n",
       " 0.99799585,\n",
       " 0.9894456,\n",
       " 0.9231528,\n",
       " 0.9982247,\n",
       " 0.9819782,\n",
       " 0.9959982,\n",
       " 0.6918397,\n",
       " 0.9669657,\n",
       " 0.98060644,\n",
       " 0.945242,\n",
       " 0.99115175,\n",
       " 0.94628525,\n",
       " 0.9228892,\n",
       " 0.99560815,\n",
       " 0.98773366,\n",
       " 0.964861,\n",
       " 0.96473604,\n",
       " 0.6118236,\n",
       " 0.99965906,\n",
       " 0.9615628,\n",
       " 0.95331,\n",
       " 0.98482096,\n",
       " 0.99668825,\n",
       " 0.89137906,\n",
       " 0.99940974,\n",
       " 0.5381627,\n",
       " 0.9847407,\n",
       " 0.90845734,\n",
       " 0.9979874,\n",
       " 0.79437155,\n",
       " 0.9945139,\n",
       " 0.87650514,\n",
       " 0.88396853,\n",
       " 0.95641744,\n",
       " 0.9977214,\n",
       " 0.9739263,\n",
       " 0.9994741,\n",
       " 0.9822654,\n",
       " 0.9994051,\n",
       " 0.99941134,\n",
       " 0.9977095,\n",
       " 0.6863924,\n",
       " 0.7637522,\n",
       " 0.99947536,\n",
       " 0.9992805,\n",
       " 0.9915873,\n",
       " 0.97857016,\n",
       " 0.9984924,\n",
       " 0.5522611,\n",
       " 0.8450444,\n",
       " 0.50462633,\n",
       " 0.5552392,\n",
       " 0.9888106,\n",
       " 0.9990741,\n",
       " 0.9972832,\n",
       " 0.99577653,\n",
       " 0.99866724,\n",
       " 0.56822455,\n",
       " 0.998989,\n",
       " 0.97912055,\n",
       " 0.9789747,\n",
       " 0.91193414,\n",
       " 0.9618203,\n",
       " 0.9391227,\n",
       " 0.9486615,\n",
       " 0.7304115,\n",
       " 0.87603754,\n",
       " 0.98226607,\n",
       " 0.9989844,\n",
       " 0.9944584,\n",
       " 0.99635065,\n",
       " 0.9840855,\n",
       " 0.8440615,\n",
       " 0.9938493,\n",
       " 0.9936167,\n",
       " 0.85574424,\n",
       " 0.5247137,\n",
       " 0.99052465,\n",
       " 0.989837,\n",
       " 0.9979793,\n",
       " 0.84703714,\n",
       " 0.9609323,\n",
       " 0.99746996,\n",
       " 0.9962841,\n",
       " 0.83012897,\n",
       " 0.7233691,\n",
       " 0.40393537,\n",
       " 0.40866706,\n",
       " 0.9500471,\n",
       " 0.6787386,\n",
       " 0.96006066,\n",
       " 0.99610096,\n",
       " 0.84618014,\n",
       " 0.92009133,\n",
       " 0.99773425,\n",
       " 0.96836054,\n",
       " 0.9365378,\n",
       " 0.9077522,\n",
       " 0.97513026,\n",
       " 0.86004144,\n",
       " 0.99582654,\n",
       " 0.9921761,\n",
       " 0.9985929,\n",
       " 0.996864,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob_las = []\n",
    "for i in range(len(pred_tuple[0])):\n",
    "    pred_prob_las.append(max(softmax(pred_tuple[-1][i])))\n",
    "pred_prob_las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u46A_275lVDO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996928"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(pred_prob_thi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_labels_1 = []\n",
    "for i in op_labels:\n",
    "    if i == 0:\n",
    "        op_labels_1.append(1)\n",
    "    elif i==1:\n",
    "        op_labels_1.append(2)\n",
    "    else:\n",
    "        op_labels_1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "gGJUAlUIH3SP"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(final_preds[2],final_preds[5], final_preds[-1], pred_prob_thi, pred_prob_six, pred_prob_las, op_labels_1)), columns =['Thi_layer_P','Six_layer_P', 'Last_layer','PProb_thi', 'PProb_six', 'PProb_las', 'True_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "H8RVPJMeXU9E"
   },
   "outputs": [],
   "source": [
    "# df1 = pd.DataFrame(list(zip(final_preds[0], final_preds[1], final_preds[2], final_preds[3], final_preds[4], final_preds[5], final_preds[6], final_preds[7], final_preds[8], final_preds[9], final_preds[10], final_preds[11], op_labels)), columns =['Fir_p', 'Sec_p', 'Thi_p', 'Fou_p', 'Fiv_p', 'Six_p', 'Sev_p', 'Eig_p', 'Nin_p', 'Ten_p', 'Ele_p', 'Twe_p', 'True_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "SJdbOqbi2AiL"
   },
   "outputs": [],
   "source": [
    "# df2 = pd.DataFrame(list(zip(final_preds[0], final_preds[1], final_preds[2], final_preds[3], final_preds[4], final_preds[5], final_preds[6], op_labels)), columns =['Fir_p', 'Thi_p', 'Fou_p', 'Fiv_p', 'Sev_p', 'Nin_p', 'Twe_p', 'True_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "eePvXaR2g44c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thi_layer_P</th>\n",
       "      <th>Six_layer_P</th>\n",
       "      <th>Last_layer</th>\n",
       "      <th>PProb_thi</th>\n",
       "      <th>PProb_six</th>\n",
       "      <th>PProb_las</th>\n",
       "      <th>True_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.983691</td>\n",
       "      <td>0.998154</td>\n",
       "      <td>0.999106</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.973306</td>\n",
       "      <td>0.999317</td>\n",
       "      <td>0.999492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409512</td>\n",
       "      <td>0.951424</td>\n",
       "      <td>0.952983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.891311</td>\n",
       "      <td>0.967909</td>\n",
       "      <td>0.969821</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.828995</td>\n",
       "      <td>0.971407</td>\n",
       "      <td>0.982470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Thi_layer_P  Six_layer_P  Last_layer  PProb_thi  PProb_six  PProb_las  \\\n",
       "0            2            2           2   0.983691   0.998154   0.999106   \n",
       "1            0            0           0   0.973306   0.999317   0.999492   \n",
       "2            2            1           1   0.409512   0.951424   0.952983   \n",
       "3            2            2           2   0.891311   0.967909   0.969821   \n",
       "4            1            1           1   0.828995   0.971407   0.982470   \n",
       "\n",
       "   True_labels  \n",
       "0            2  \n",
       "1            0  \n",
       "2            1  \n",
       "3            2  \n",
       "4            1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gfj1nbWqfea4",
    "outputId": "967cc785-e147-4b62-a3e5-29c02a8df442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7347419848662188\n",
      "Accuracy =  0.7943032617539824\n",
      "Accuracy =  0.7971337921644365\n",
      "Accuracy =  0.0\n",
      "Accuracy =  0.0\n",
      "Accuracy =  0.0\n",
      "Accuracy =  1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy_imdb = []\n",
    "for j in df.columns:\n",
    "  accuracy = 0\n",
    "  for i in range(df.shape[0]):\n",
    "      if df[j][i] == df['True_labels'][i]:\n",
    "          accuracy += 1\n",
    "      else:\n",
    "          pass\n",
    "  print(\"Accuracy = \", accuracy/df.shape[0])\n",
    "  accuracy_imdb.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "piV1ZUbg1zhc",
    "outputId": "70c2daee-416d-4598-8ff0-8aaf349d899f"
   },
   "outputs": [],
   "source": [
    "# accuracy_yelp = []\n",
    "# for j in df2.columns:\n",
    "#   accuracy = 0\n",
    "#   for i in range(df1.shape[0]):\n",
    "#       if df2[j][i] == df2['True_labels'][i]:\n",
    "#           accuracy += 1\n",
    "#       else:\n",
    "#           pass\n",
    "#   print(\"Accuracy = \", accuracy/df2.shape[0])\n",
    "#   accuracy_imdb.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "mgX5cPgt8vZr"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/divya/UBERT/CSV_files/Early_Exit_Confidence_data_snli_max_exits(3,6,12)_difference.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "dE2200MMhJz1"
   },
   "outputs": [],
   "source": [
    "# df.to_csv(r\"/UBERT/CSV_files/Early_Exit_Confidence_data_SST2_new_exits(12,12)_difference.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOpSU7ShmMU6",
    "outputId": "34d6d257-a43b-4d4f-d2d3-9396cf5fa963"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [03:11<00:00,  4.08it/s]\n",
      "Evaluating: 100%|██████████| 782/782 [03:10<00:00,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6  7  op_labels\n",
      "0  0  0  0  1  0  0  0  1          1\n",
      "1  0  0  0  1  0  0  0  0          1\n",
      "2  0  0  0  1  0  0  0  0          1\n",
      "3  0  0  0  1  0  0  1  1          1\n",
      "4  0  0  1  1  0  0  1  1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = get_preds(eval_dataset=dataset, data_split='train')\n",
    "df_test = get_preds(eval_dataset=dataset, data_split='test')\n",
    "\n",
    "df_tot = pd.concat([df_train, df_test])\n",
    "df_tot = df_tot.reset_index(drop=True)\n",
    "print(df_tot.head())\n",
    "\n",
    "df_tot.to_csv(r'/content/drive/MyDrive/Early_Exits_Divya/Model_exit_predictions/Exit_Predictions_TrainTest_IMDb_8exits.csv',sep ='\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlgCgxMQm17O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
