{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZrkmpkW8Wbj"
      },
      "source": [
        "# Part-1 : Section A\n",
        "Training a multi-exit ElasticBERT model on SST-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CkfzdLG9C1T",
        "outputId": "3edbf1a5-d5a8-42d9-f15d-5a7aa827949d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MutiExitDNNs'...\n",
            "remote: Enumerating objects: 179, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 179 (delta 58), reused 0 (delta 0), pack-reused 80\u001b[K\n",
            "Receiving objects: 100% (179/179), 2.27 MiB | 10.43 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "[Errno 2] No such file or directory: '/content/MutiExitDNNs/ElasticBERT'\n",
            "/content/ElasticBERT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.6.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (4.6.1)\n",
            "Requirement already satisfied: fitlog in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.9.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->-r requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->-r requirements.txt (line 1)) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->-r requirements.txt (line 1)) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->-r requirements.txt (line 1)) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->-r requirements.txt (line 1)) (3.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->-r requirements.txt (line 1)) (0.0.53)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from fitlog->-r requirements.txt (line 2)) (1.1.4)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from fitlog->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: gitpython>=3.1.2 in /usr/local/lib/python3.8/dist-packages (from fitlog->-r requirements.txt (line 2)) (3.1.30)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask>=1.0.2->fitlog->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.0.2->fitlog->-r requirements.txt (line 2)) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask>=1.0.2->fitlog->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.0.2->fitlog->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython>=3.1.2->fitlog->-r requirements.txt (line 2)) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.2->fitlog->-r requirements.txt (line 2)) (5.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.0.2->fitlog->-r requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.6.1->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.6.1->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.6.1->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.6.1->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.6.1->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 1)) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# The code closely follows the original ElasticBERT repository\n",
        "# Feature to train models with a given exit configuration is added\n",
        "!git clone https://github.com/MLiONS/MutiExitDNNs.git\n",
        "\n",
        "\n",
        "%cd /content/MutiExitDNNs/ElasticBERT\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z-4ln4kltx_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f798a8b-3753-461c-c34a-a5be994a885e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A26ZgAoxPyiJ",
        "outputId": "f22070d2-c450-4215-b6ba-7779b509b8d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: /content/MutiExitDNNs/ElasticBERT/finetune-dynamic/finetune_elue_entropy.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#All the hyper-parameters/ location to training dataset are set in\n",
        "#MultiExitDNNs -> finetune-dynamic -> finetune_elue_entropy.sh file\n",
        "\n",
        "#1)Set the correct location to SST-2 dataset\n",
        "#All models are trained on SST-2 \"train\" split and evaluated on \"dev\" split\n",
        "#\"train.tsv\" and \"dev.tsv\" are expected to be in ELUE_DIR/TASK_NAME\n",
        "#You can set both ELUE_DIR and TASK_NAME in finetune_elue_entropy.sh\n",
        "#Or change the dataset directory using \"data_dir\" option\n",
        "\n",
        "#2)Please change the \"num_output_layers\" option as per the desired exit-configuration\n",
        "\n",
        "#3)Model checkpoints will be saved at \"output_dir\" and\n",
        "#logs will be available at \"log_dir\"\n",
        "\n",
        "!bash /content/MutiExitDNNs/ElasticBERT/finetune-dynamic/finetune_elue_entropy.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tfcWxU28kkl"
      },
      "source": [
        "# Part-1 : Section B\n",
        "Generating the prediction matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acoXLlG2y1ty"
      },
      "outputs": [],
      "source": [
        "#Evaluation on other datasets-IMDb or Yelp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb9mBUI5y1q-",
        "outputId": "cf3257c4-a6a8-4c24-98a4-614a1370443b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MutiExitDNNs/ElasticBERT/finetune-dynamic\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer as ElasticBertTokenizer\n",
        "\n",
        "#Set the current directory location inside \"finetune-dynamic\" folder\n",
        "%cd /content/MutiExitDNNs/ElasticBERT/finetune-dynamic\n",
        "\n",
        "from models.configuration_elasticbert import ElasticBertConfig\n",
        "from models.modeling_elasticbert_entropy import ElasticBertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkT2IrL2b90d"
      },
      "outputs": [],
      "source": [
        "#Set location to the best performing model\n",
        "#Model checkpoints are saved at \"output_dir\" from Part-1: Section A\n",
        "\n",
        "checkpoint = '/content/MutiExitDNNs/ElasticBERT/ckpts/elue/entropy/SST-2/checkpoint-50'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCv-3N0CaM0s"
      },
      "outputs": [],
      "source": [
        "config = ElasticBertConfig.from_pretrained(checkpoint)\n",
        "tokenizer = ElasticBertTokenizer.from_pretrained(checkpoint)\n",
        "model = ElasticBertForSequenceClassification.from_pretrained(checkpoint)\n",
        "#model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFt7llc_g130"
      },
      "outputs": [],
      "source": [
        "def get_args(arg_vec):\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\n",
        "        \"--num_hidden_layers\",\n",
        "        default=None,\n",
        "        type=int,\n",
        "        required=True,\n",
        "        help='The number of layers to import.',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_output_layers\",\n",
        "        nargs = 12,\n",
        "        default=None,\n",
        "        type=int,\n",
        "        required=True,\n",
        "        help='The number of layers to output.',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to pre-trained model or shortcut name.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--task_name\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The name of the task to train selected in the list.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the logs will be written.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--spec_eval\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=False,\n",
        "        help=\"'Set as train or test based on specific split on which to evaluate'\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--patience\",\n",
        "        default='0',\n",
        "        type=str,\n",
        "        required=False,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--regression_threshold\",\n",
        "        default=0,\n",
        "        type=float,\n",
        "        required=False,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--early_exit_entropy\",\n",
        "        default='0.1',\n",
        "        type=str,\n",
        "        required=False,\n",
        "    )\n",
        "    # Other parameters\n",
        "    parser.add_argument(\n",
        "        \"--load\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"The path of ckpts used to continue training.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--config_name\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Pretrained config name or path if not the same as model_name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer_name\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cache_dir\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_seq_length\",\n",
        "        default=128,\n",
        "        type=int,\n",
        "        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "             \"than this will be truncated, sequences shorter will be padded.\",\n",
        "    )\n",
        "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Whether to use debug mode.\")\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\n",
        "        \"--evaluate_during_training\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Run evaluation during training at each logging step.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_lower_case\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Set this flag if you are using an uncased model.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_train_batch_size\",\n",
        "        default=8,\n",
        "        type=int,\n",
        "        help=\"Batch size per GPU/CPU for training.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_eval_batch_size\",\n",
        "        default=1,\n",
        "        type=int,\n",
        "        help=\"Batch size per GPU/CPU for evaluation.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--learning_rate\",\n",
        "        default=5e-5,\n",
        "        type=float,\n",
        "        help=\"The initial learning rate for Adam.\",\n",
        "    )\n",
        "    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\",\n",
        "        default=3.0,\n",
        "        type=float,\n",
        "        help=\"Total number of training epochs to perform.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_steps\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
        "    )\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "    parser.add_argument(\"--warmup_rate\", default=0, type=float, help=\"Linear warmup over warmup_rate.\")\n",
        "\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--save_steps\",\n",
        "        type=int,\n",
        "        default=500,\n",
        "        help=\"Save checkpoint every X updates steps.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_all_checkpoints\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
        "    )\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_output_dir\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Overwrite the content of the output directory\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_cache\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Overwrite the cached training and evaluation sets\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--not_save_model\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Do not save model checkpoints\"\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=6, help=\"random seed for initialization\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "             \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--local_rank\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        help=\"For distributed training: local_rank\",\n",
        "    )\n",
        "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    args = parser.parse_args(arg_vec)\n",
        "\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2cX_ym5pJwl"
      },
      "outputs": [],
      "source": [
        "from load_data import (\n",
        "    load_and_cache_examples_glue,\n",
        "    load_and_cache_examples_elue,\n",
        ")\n",
        "\n",
        "def evaluate_elue_entropy(args, model, tokenizer, prefix=\"\", eval_highway=False, entropy=0.):\n",
        "    model.elasticbert.set_early_exit_entropy(entropy)\n",
        "    model.elasticbert.set_eval_state(eval_highway)\n",
        "    model.elasticbert.reset_stats()\n",
        "\n",
        "    eval_task = args.task_name.lower()\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    num_op_layers = args.num_output_layers\n",
        "\n",
        "    results = {}\n",
        "    results_all = []\n",
        "    exit_layer = []\n",
        "    for i in range(sum(num_op_layers)):\n",
        "        results_all.append({})\n",
        "\n",
        "    if args.spec_eval:\n",
        "      eval_dataset = load_and_cache_examples_elue(args, eval_task, tokenizer, data_type=args.spec_eval)\n",
        "    else:\n",
        "      eval_dataset = load_and_cache_examples_elue(args, eval_task, tokenizer, data_type='dev')\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu eval\n",
        "    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    preds_all = []\n",
        "    pred_tuple = []\n",
        "    for i in range(sum(num_op_layers)):\n",
        "        preds_all.append(None)\n",
        "        pred_tuple.append(None)\n",
        "    out_label_ids = None\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"labels\": batch[-1],\n",
        "            }\n",
        "            inputs[\"token_type_ids\"] = batch[2]\n",
        "            outputs = model(**inputs)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "        if out_label_ids is None:\n",
        "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "        else:\n",
        "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "        if not eval_highway:\n",
        "            for i, pred in enumerate(preds_all):\n",
        "                if pred is None:\n",
        "                    preds_all[i] = logits[i].detach().cpu().numpy()\n",
        "                else:\n",
        "                    preds_all[i] = np.append(pred, logits[i].detach().cpu().numpy(), axis=0)\n",
        "        else:\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    if args.output_mode == \"classification\":\n",
        "        if not eval_highway:\n",
        "            for i, pred in enumerate(preds_all):\n",
        "                preds_all[i] = np.argmax(pred, axis = 1)\n",
        "                pred_tuple[i] = pred\n",
        "        else:\n",
        "            preds = np.argmax(preds, axis = 1)\n",
        "            pred_tuple[i] = pred\n",
        "\n",
        "    elif args.output_mode == \"regression\":\n",
        "        if not eval_highway:\n",
        "            for i, pred in enumerate(preds_all):\n",
        "                preds_all[i] = np.squeeze(pred)\n",
        "        else:\n",
        "            preds = np.squeeze(preds)\n",
        "\n",
        "    if not eval_highway:\n",
        "        for i, pred in enumerate(preds_all):\n",
        "            if eval_task == 'yelp':\n",
        "              eval_task = 'imdb'\n",
        "            result = elue_compute_metrics(eval_task, pred, out_label_ids)\n",
        "            results_all[i].update(result)\n",
        "\n",
        "    else:\n",
        "        result = elue_compute_metrics(eval_task, preds, out_label_ids)\n",
        "        results.update(result)\n",
        "\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            print(\"  %s = %s\" % (key, str(result[key])))\n",
        "\n",
        "        exiting_layer_every_ins = model.elasticbert.exiting_layer_every_ins\n",
        "        exit_layer.append(exiting_layer_every_ins)\n",
        "\n",
        "    if eval_highway:\n",
        "        speed_up = model.elasticbert.log_stats()\n",
        "        return results, speed_up, exit_layer\n",
        "\n",
        "    if args.spec_eval:\n",
        "      return results_all, preds_all, pred_tuple, out_label_ids\n",
        "\n",
        "    return results_all, preds_all, pred_tuple , output_label_ids\n",
        "    #return results_all, preds_all, out_label_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCJmL5-yez21"
      },
      "outputs": [],
      "source": [
        "ELUE_DIR='/content/drive/MyDrive/elue_data'\n",
        "TASK_NAME='SST-2'\n",
        "\n",
        "arg_vec= ['--model_name_or_path', 'fnlp/elasticbert-base',\n",
        "  '--task_name', 'SST-2', \\\n",
        "  '--do_train', \\\n",
        "  '--do_lower_case', \\\n",
        "  '--data_dir', \"/content/drive/MyDrive/elue_data/STS-B\", \\\n",
        "  '--log_dir', '/content/ElasticBERT/logs/elue/entropy/STS-BTestCheck', \\\n",
        "  '--output_dir', '/content/ElasticBERT/ckpts/elue/entropy/STS-BTestCheck', \\\n",
        "  '--num_hidden_layers', '12', \\\n",
        "  '--num_output_layers', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', \\\n",
        "  '--max_seq_length', '128', \\\n",
        "  '--per_gpu_train_batch_size', '32', \\\n",
        "  '--per_gpu_eval_batch_size',' 32', \\\n",
        "  '--learning_rate', '2e-5', \\\n",
        "  '--weight_decay', '0.1', \\\n",
        "  '--save_steps', '50', \\\n",
        "  '--logging_steps', '50', \\\n",
        "  '--num_train_epochs', '5',  \\\n",
        "  '--warmup_rate', '0.06', \\\n",
        "  '--evaluate_during_training', \\\n",
        "  '--overwrite_output_dir'\n",
        "]\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "args = get_args(arg_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLZpjqakoP9G",
        "outputId": "1d33a2cc-fd03-482e-bf3c-e83d7ce3e252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElasticBertForSequenceClassification(\n",
              "  (elasticbert): ElasticBertModel(\n",
              "    (embeddings): ElasticBertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): ElasticBertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): ElasticBertLayer(\n",
              "          (attention): ElasticBertAttention(\n",
              "            (self): ElasticBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElasticBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElasticBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElasticBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-08, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): ModuleList(\n",
              "        (0): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (1): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (2): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (3): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (4): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (5): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (6): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (7): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (8): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (9): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (10): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "        (11): ElasticBertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifiers): ModuleList(\n",
              "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (1): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (2): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (3): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (4): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (5): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (6): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (7): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (8): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (9): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (10): Linear(in_features=768, out_features=2, bias=True)\n",
              "    (11): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import torch\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend=\"nccl\")\n",
        "    args.n_gpu = 1\n",
        "args.device = device\n",
        "\n",
        "args.output_mode = 'classification'\n",
        "\n",
        "print(args.device)\n",
        "model.to(args.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50EjaE5MhPkU"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/hsm207/imdb_data.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfKacfaChWSJ"
      },
      "outputs": [],
      "source": [
        "# %cd imdb_data\n",
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsgrccgxiiQq"
      },
      "outputs": [],
      "source": [
        "# !tf_upgrade_v2 --infile create_imdb_dataset.py --outfile bar.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbLkqld8ipqI"
      },
      "outputs": [],
      "source": [
        "# !python bar.py --output_dir /content/imdb_data/imdb_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNSuHnH-mSPt"
      },
      "outputs": [],
      "source": [
        "#Custom Selection\n",
        "dataset = 'SST-2'#'IMDb' #or 'Yelp'\n",
        "\n",
        "#To check model performance on SST-2 dev split:\n",
        "#Please set dataset = 'SST-2' and data_split='dev'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyrFjs52hwRW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_preds(eval_dataset='SST-2', data_split='train'):\n",
        "  args.spec_eval=data_split\n",
        "  args.task_name=eval_dataset.lower()\n",
        "  args.data_dir=ELUE_DIR + '/'+args.task_name\n",
        "\n",
        "  results_all, exit_preds, pred_tuple, op_labels = evaluate_elue_entropy(args, model, tokenizer)\n",
        "\n",
        "\n",
        "  # exit_preds_list = np.stack(exit_preds, axis=1)\n",
        "  # df = pd.DataFrame((exit_preds_list) )\n",
        "  # df['op_labels'] = op_labels\n",
        "\n",
        "  return  results_all, exit_preds, pred_tuple, op_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxJkyv_xHy-H",
        "outputId": "72d00b18-fa0a-45fe-e857-233319825ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 267/267 [01:03<00:00,  4.20it/s]\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import os\n",
        "from elue import elue_compute_metrics\n",
        "from tqdm import tqdm\n",
        "results, final_preds, pred_tuple, op_labels = get_preds(eval_dataset=dataset, data_split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqNuO_askX0X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "def softmax(x):\n",
        "    return(np.exp(x)/np.exp(x).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX8DlkDGWdQ-",
        "outputId": "639a3136-cd41-4455-e370-1d02f7e01204"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.07783183, -0.14257424],\n",
              "       [-0.08552232, -0.02014458],\n",
              "       [ 0.23074193, -0.29304874],\n",
              "       ...,\n",
              "       [ 0.14448684, -0.32708505],\n",
              "       [ 0.05847542, -0.17466402],\n",
              "       [ 0.20079498, -0.2604479 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "pred_tuple[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u46A_275lVDO"
      },
      "outputs": [],
      "source": [
        "pred_prob_thi = []\n",
        "pred_prob_las = []\n",
        "\n",
        "\n",
        "for i in range(len(pred_tuple[0])):\n",
        "  pred_prob_thi.append(abs((pred_tuple[1][i][0]-pred_tuple[1][i][1])))\n",
        "  pred_prob_las.append(abs((pred_tuple[-1][i][0]-pred_tuple[-1][i][1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGJUAlUIH3SP"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(list(zip(final_preds[1], final_preds[-1], pred_prob_thi, pred_prob_las, op_labels)), columns =['Thi_layer_P', 'Last_layer', 'PProb_Thi', 'PProb_las', 'True_labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8RVPJMeXU9E"
      },
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame(list(zip(final_preds[0], final_preds[1], final_preds[2], final_preds[3], final_preds[4], final_preds[5], final_preds[6], final_preds[7], final_preds[8], final_preds[9], final_preds[10], final_preds[11], op_labels)), columns =['Fir_p', 'Sec_p', 'Thi_p', 'Fou_p', 'Fiv_p', 'Six_p', 'Sev_p', 'Eig_p', 'Nin_p', 'Ten_p', 'Ele_p', 'Twe_p', 'True_labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJdbOqbi2AiL"
      },
      "outputs": [],
      "source": [
        "df2 = pd.DataFrame(list(zip(final_preds[0], final_preds[1], final_preds[2], final_preds[3], final_preds[4], final_preds[5], final_preds[6], op_labels)), columns =['Fir_p', 'Thi_p', 'Fou_p', 'Fiv_p', 'Sev_p', 'Nin_p', 'Twe_p', 'True_labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eePvXaR2g44c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfj1nbWqfea4",
        "outputId": "967cc785-e147-4b62-a3e5-29c02a8df442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.50584\n",
            "Accuracy =  0.52\n",
            "Accuracy =  0.55244\n",
            "Accuracy =  0.551\n",
            "Accuracy =  0.62064\n",
            "Accuracy =  0.60052\n",
            "Accuracy =  0.77208\n",
            "Accuracy =  1.0\n"
          ]
        }
      ],
      "source": [
        "accuracy_imdb = []\n",
        "for j in df1.columns:\n",
        "  accuracy = 0\n",
        "  for i in range(df1.shape[0]):\n",
        "      if df1[j][i] == df1['True_labels'][i]:\n",
        "          accuracy += 1\n",
        "      else:\n",
        "          pass\n",
        "  print(\"Accuracy = \", accuracy/df1.shape[0])\n",
        "  accuracy_imdb.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "piV1ZUbg1zhc",
        "outputId": "70c2daee-416d-4598-8ff0-8aaf349d899f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-8efe289fc34a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'True_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
          ]
        }
      ],
      "source": [
        "accuracy_yelp = []\n",
        "for j in df2.columns:\n",
        "  accuracy = 0\n",
        "  for i in range(df1.shape[0]):\n",
        "      if df2[j][i] == df2['True_labels'][i]:\n",
        "          accuracy += 1\n",
        "      else:\n",
        "          pass\n",
        "  print(\"Accuracy = \", accuracy/df2.shape[0])\n",
        "  accuracy_imdb.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgX5cPgt8vZr"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"Early_Exit_Confidence_data_yelp_new_exits(3,12)_difference.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE2200MMhJz1"
      },
      "outputs": [],
      "source": [
        "df1.to_csv(\"Early_Exit_Confidence_data_SST2_new_exits(12,12)_difference.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx2JCloBH8MV"
      },
      "outputs": [],
      "source": [
        "df.to_csv(r'/content/drive/MyDrive/elue_data/Exit_Predictions_TrainTest_Yelp_4exits.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svv4Ov77V_qM",
        "outputId": "4b17a2af-5ed1-4316-abde-27fbde8b2e7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8544, 5)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7t9t7HjOGOl"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjtOdVKvOOJY"
      },
      "outputs": [],
      "source": [
        "# files.download('/content/MutiExitDNNs/ElasticBERT/finetune-dynamic/Early_Exit_Confidence_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOpSU7ShmMU6",
        "outputId": "34d6d257-a43b-4d4f-d2d3-9396cf5fa963"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|| 782/782 [03:11<00:00,  4.08it/s]\n",
            "Evaluating: 100%|| 782/782 [03:10<00:00,  4.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   0  1  2  3  4  5  6  7  op_labels\n",
            "0  0  0  0  1  0  0  0  1          1\n",
            "1  0  0  0  1  0  0  0  0          1\n",
            "2  0  0  0  1  0  0  0  0          1\n",
            "3  0  0  0  1  0  0  1  1          1\n",
            "4  0  0  1  1  0  0  1  1          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "df_train = get_preds(eval_dataset=dataset, data_split='train')\n",
        "df_test = get_preds(eval_dataset=dataset, data_split='test')\n",
        "\n",
        "df_tot = pd.concat([df_train, df_test])\n",
        "df_tot = df_tot.reset_index(drop=True)\n",
        "print(df_tot.head())\n",
        "\n",
        "df_tot.to_csv(r'/content/drive/MyDrive/Early_Exits_Divya/Model_exit_predictions/Exit_Predictions_TrainTest_IMDb_8exits.csv',sep ='\\t', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlgCgxMQm17O"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}